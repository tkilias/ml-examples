{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Example Using Exasol\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example demonstrates how machine learning techniques can be used inside Exasol.\n",
    "\n",
    "It was created using the publicly available dataset from the Industrial Challenge at the [15th International Symposium on Intelligent Data Analysis](http://ida2016.blogs.dsv.su.se/) (IDA) in 2016. The dataset, provided by Scania CV AB, consists of real data from heavy Scania trucks during normal operation. For details, please see the data description file provided with the data.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This example is to show how you can use machine learning tools with Exasol. Therefore, we will not discuss the machine learning details such as classifier selection and tuning. Because there are many different machine learning methods, choosing a \"good\" one is highly dependent on the problem to be solved and the data. Rather, we want to demonstrate how you can more effectively use <b>your models</b> with <b>your data</b> in Exasol.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The intended audience of this article is assumed to have a basic understanding of the following.\n",
    "* Exasol, in particular UDFs\n",
    "* Machine learning methods\n",
    "* Python programming, including\n",
    "  * Scikit-learn\n",
    "  * Pandas\n",
    "  * NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The purpose of the challenge was to best predict which failures are related to a specific component of a truck's air pressure system (APS) as opposed to failures unrelated to the APS. Specifically, the following cost metric was given, which was to be minimized.\n",
    "\n",
    "$$cost_{total}=cost_{FP}\\cdot{FP} + cost_{FN}\\cdot{FN}$$\n",
    "where  \n",
    "$FP$ is the number of false positives (predicted APS failure, but really isn't),  \n",
    "$FN$ is the number of false negatives (predicted non-APS failure, but really is),  \n",
    "$cost_{FP}=10$ is the cost of an unnecessary check by a mechanic, and  \n",
    "$cost_{FN}=500$ is the cost of not checking a faulty truck and possibly causing a breakdown.\n",
    "\n",
    "So from the cost metric, we can see that a preventative check is much cheaper (50x) than a repair after a breakdown, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Here, we specify some basic information, which is used throughout this example. In particular, the location, user, and password for the Exasol host(s) and EXABucket are specified, as well as the scripting language to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXASOL_HOST = \"localhost:8563\"\n",
    "EXASOL_USER = \"sys\"\n",
    "EXASOL_PASSWORD = \"pw\"\n",
    "EXASOL_BUCKETFS_HOST = \"localhost:2581\"\n",
    "EXASOL_BUCKETFS_USER = \"w\"\n",
    "EXASOL_BUCKETFS_PASSWORD = \"pw\"\n",
    "EXASOL_SCRIPT_LANGUAGES = \"SKLEARN_PYTHON=localzmq+protobuf:///bucketfs1/udfs/scikit_learn?lang=python#buckets/bucketfs1/udfs/scikit_learn/exaudf/exaudfclient\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data\n",
    "\n",
    "To begin, we download the [IDA 2016 Challenge dataset](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) (20MB) from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) and import it into training and test DataFrames.\n",
    "\n",
    "Because the ZIP file contains a data description file and both the training and test data, which must be kept separate, we cannot import the file directly into Exasol using Exasol's IMPORT statement. Instead, we first read the two data files into separate local DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00414/to_uci.zip\"\n",
    "TRAINING_FILE = \"to_uci/aps_failure_training_set.csv\"\n",
    "TEST_FILE = \"to_uci/aps_failure_test_set.csv\"\n",
    "\n",
    "# Data is preceeded with a 20-line header (copyright & license)\n",
    "NUM_SKIP_ROWS = 20\n",
    "NA_VALUE = \"na\"\n",
    "\n",
    "resp = urlopen(DATA_URL)\n",
    "with ZipFile(BytesIO(resp.read())) as z:\n",
    "    train_set = pd.read_csv(TRAINING_FILE, skiprows=NUM_SKIP_ROWS, na_values=NA_VALUE)\n",
    "    test_set = pd.read_csv(TEST_FILE, skiprows=NUM_SKIP_ROWS, na_values=NA_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data into Exasol\n",
    "\n",
    "We can now load the data from the DataFrames into Exasol tables. By having a quick look at the data and/or reading the provided data description file, we can see that the first data column is the class label ('neg'/'pos') and can be stored in a `VARCHAR(3)` column. The other data columns are all numerical features which can be stored in `DECIMAL(18, 2)` columns.\n",
    "\n",
    "For this example, we create an Exasol schema named `IDA`, in which everything will be stored. Then, we load the training and test data from the DataFrames into two tables named `TRAIN` and `TEST`, respectively. For this step, we use the very convenient [pyexasol](https://github.com/badoo/pyexasol) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "# Create schema\n",
    "conn.execute(\"CREATE SCHEMA IF NOT EXISTS IDA\")\n",
    "conn.execute(\"OPEN SCHEMA IDA\")\n",
    "\n",
    "# Create tables for data\n",
    "column_names = list(train_set.columns)\n",
    "column_types = [\"VARCHAR(3)\"] + [\"DECIMAL(18,2)\"] * (len(column_names) - 1)\n",
    "column_desc = [\" \".join(t) for t in zip(column_names, column_types)]\n",
    "\n",
    "conn.execute(\"CREATE OR REPLACE TABLE IDA.TRAIN(\" + \", \".join(column_desc) + \")\")\n",
    "conn.execute(\"CREATE OR REPLACE TABLE IDA.TEST LIKE IDA.TRAIN\")\n",
    "\n",
    "# Import data into Exasol\n",
    "conn.import_from_pandas(train_set, \"TRAIN\")\n",
    "print(f\"Imported {conn.last_statement().rowcount()} rows into IDA.TRAIN.\")\n",
    "conn.import_from_pandas(test_set, \"TEST\")\n",
    "print(f\"Imported {conn.last_statement().rowcount()} rows into IDA.TEST.\")\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Data\n",
    "\n",
    "After loading the data into Exasol, we may first want to get a feel for the data before creating a classifier. There are many different ways to do so, such as data visualization, viewing basic statistical information, examining feature correlation, etc.\n",
    "\n",
    "In the example below, we examine the training data's basic statistical information using `pandas.DataFrame.describe()` and `pandas.DataFrame.var()` and output the combinded results for the first five columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(f\"ALTER SESSION SET SCRIPT_LANGUAGES='{EXASOL_SCRIPT_LANGUAGES}'\")\n",
    "\n",
    "# Create script output column descriptions\n",
    "# Numeric data\n",
    "out_column_types = [\"DOUBLE\"] * len(column_names)\n",
    "out_column_desc = [\" \".join(t) for t in zip(column_names, out_column_types)]\n",
    "\n",
    "# Create script to run pandas.DataFrame.describe() and pandas.DataFrame.var() in Exasol\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE SKLEARN_PYTHON SET SCRIPT IDA.DF_DESCRIBE({\", \".join(column_desc)})\n",
    "EMITS ({\", \".join(out_column_desc)}) AS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "def create_df(ctx):\n",
    "    df = []\n",
    "    col_names = [col.name for col in exa.meta.input_columns]\n",
    "    while True:\n",
    "        row = []\n",
    "        for i in range(0, exa.meta.input_column_count):\n",
    "            row.append(ctx[i])\n",
    "        df.append(row)\n",
    "        if not ctx.next():\n",
    "            break;\n",
    "    df = pd.DataFrame(df, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Convert NumPy value to native Python type\n",
    "def np_numeric_as_scalar(x):\n",
    "    return [np.asscalar(x[i]) for i in range(0, len(x))]\n",
    "\n",
    "def get_stats(X):\n",
    "    # Replace 'neg'/'pos' with 0/1\n",
    "    X.loc[:, 'class'] = X.loc[:, 'class'].replace({{'neg': 0, 'pos': 1}})\n",
    "    # Convert all columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    # Get DataFrame stats\n",
    "    X_describe = X.describe()\n",
    "\n",
    "    # Get DataFrame variance\n",
    "    X_var = X.var()\n",
    "    X_var.name = 'var'\n",
    "\n",
    "    # Append variance to stats\n",
    "    return X_describe.append(X_var)\n",
    "\n",
    "def run(ctx):\n",
    "    df = create_df(ctx)\n",
    "\n",
    "    # Calculate statistics info\n",
    "    df = get_stats(df)\n",
    "\n",
    "    # Output data description\n",
    "    for i in range(0, df.shape[0]):\n",
    "        out_list = np_numeric_as_scalar(df.iloc[i, :])\n",
    "        ctx.emit(*out_list)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Create table \"TRAIN_DESCRIPTION\" to hold the description output\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE TABLE IDA.TRAIN_DESCRIPTION AS\n",
    "    SELECT IDA.DF_DESCRIBE({\", \".join(column_names)}) from IDA.TRAIN\n",
    "\"\"\")\n",
    "conn.execute(sql)\n",
    "\n",
    "# Create local data frame from the \"TRAIN_DESCRIPTION\" table\n",
    "train_desc = conn.export_to_pandas(\"SELECT * FROM IDA.TRAIN_DESCRIPTION\")\n",
    "train_desc.index = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'var']\n",
    "\n",
    "# Print first 5 columns, for example\n",
    "print(train_desc.iloc[:, 0:5])\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Data\n",
    "\n",
    "Looking at the statistics summary from the previous step, we can see, for example, that some features have missing values and that the means and variances of the features differ greatly. Because of this, it is most likely a good idea to transform the data.\n",
    "\n",
    "Depending on which classifier one plans to use, among other things, there are many different techniques one may use to transform the data, such as feature scaling and extraction.\n",
    "\n",
    "In this example, we first use imputation to replace missing values with the median value of that feature. Then, we scale the data such that each feature is normally distributed (with zero mean and unit variance). These are very simple transformations that work fairly well with many learning algorithms.\n",
    "\n",
    "In the script, the transformer (actually a transform pipeline), is fitted to the training data and then used to transform it. Then, the transformer object is saved to the specified EXABucket for future use. Finally, the transformed training data is outputted and stored in the table `TRAIN_TRANSFORMED`.\n",
    "\n",
    "The same script is then called again with the test data. Since the test data should be transformed exactly as the training data, the transformer which was previously saved is simply loaded from the EXABucket and used to transform the test data. The transformed test data is outputted and stored in the table `TEST_TRANSFORMED`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(f\"ALTER SESSION SET SCRIPT_LANGUAGES='{EXASOL_SCRIPT_LANGUAGES}'\")\n",
    "\n",
    "# Create script output column descriptions\n",
    "# One class label, numeric data\n",
    "out_column_types = [\"INT\"] + [\"DOUBLE\"] * (len(column_names) - 1)\n",
    "out_column_desc = [\" \".join(t) for t in zip(column_names, out_column_types)]\n",
    "\n",
    "# Specify EXABucket service (for storing the transformer)\n",
    "bucketfs_service = \"/buckets/bucketfs1\"\n",
    "# Specify transformer path in the EXABucket service\n",
    "transformer_path = \"/udfs/transform_pipeline.pkl\"\n",
    "\n",
    "# Create script to run pandas.DataFrame.describe() and pandas.DataFrame.var() in Exasol\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE SKLEARN_PYTHON SET SCRIPT\n",
    "IDA.DF_TRANSFORM(fit_transformer BOOL, transformer_path VARCHAR(200), {\", \".join(column_desc)})\n",
    "EMITS ({\", \".join(out_column_desc)}) AS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycurl\n",
    "import uuid\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create DataFrame\n",
    "def create_df(ctx, start_col, num_cols):\n",
    "    df = []\n",
    "    col_names = [col.name for col in exa.meta.input_columns[start_col : num_cols + num_cols]]\n",
    "    while True:\n",
    "        row = []\n",
    "        for i in range(start_col, start_col + num_cols):\n",
    "            row.append(ctx[i])\n",
    "        df.append(row)\n",
    "        if not ctx.next():\n",
    "            break;\n",
    "    df = pd.DataFrame(df, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Upload file to EXABucket\n",
    "def upload_file_bucketfs(object, path):\n",
    "    temp_file = \"/tmp/\" + str(uuid.uuid4().hex + \".pkl\")\n",
    "    joblib.dump(object, temp_file, compress=True)\n",
    "\n",
    "    with open(temp_file, \"rb\") as f:\n",
    "        url = \"https://{EXASOL_BUCKETFS_USER}:{EXASOL_BUCKETFS_PASSWORD}@{EXASOL_BUCKETFS_HOST}\" + path\n",
    "        curl = pycurl.Curl()\n",
    "        curl.setopt(pycurl.URL, url)\n",
    "        curl.setopt(pycurl.SSL_VERIFYPEER, 0)   \n",
    "        curl.setopt(pycurl.SSL_VERIFYHOST, 0)\n",
    "        curl.setopt(curl.UPLOAD, 1)\n",
    "        curl.setopt(curl.READDATA, f)\n",
    "        curl.perform()\n",
    "        curl.close()\n",
    "\n",
    "# Convert NumPy values to native Python type\n",
    "def np_numeric_as_scalar(x):\n",
    "    return [np.asscalar(x[i]) for i in range(0, len(x))]\n",
    "\n",
    "# Transform DataFrame\n",
    "def transform_dataframe(X, class_col_name, fit_transformer=False, save_path=None):\n",
    "    y = X.loc[:, class_col_name]\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Replace 'neg'/'pos' with 0/1\n",
    "    y = y.replace({{'neg': 0, 'pos': 1}})\n",
    "\n",
    "    # Convert columns to numeric data types\n",
    "    X_data = X_data.apply(pd.to_numeric)\n",
    "\n",
    "    if fit_transformer:\n",
    "        # Fit transformer and transform data\n",
    "        transformer = Pipeline ([\n",
    "            ('imputer', Imputer(strategy=\"median\")),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        X_data_transformed = transformer.fit_transform(X_data)\n",
    "        if save_path:\n",
    "            # Save transformer\n",
    "            upload_file_bucketfs(transformer, save_path)\n",
    "    else:\n",
    "        # Load transformer and transform data\n",
    "        transformer = joblib.load(save_path)\n",
    "        X_data_transformed = transformer.transform(X_data)\n",
    "\n",
    "    # Create transformed DataFrame with column names\n",
    "    y_df = pd.DataFrame(y, columns=[class_col_name])\n",
    "    X_data_df = pd.DataFrame(X_data_transformed, columns=X.columns[X.columns != class_col_name])\n",
    "    return y_df.join(X_data_df)\n",
    "\n",
    "def run(ctx):\n",
    "    # Input arguments\n",
    "    fit_transformer = ctx.fit_transformer\n",
    "    transformer_path = ctx.transformer_path\n",
    "\n",
    "    # Specify number of non-data (above) and data (everything else) columns\n",
    "    num_non_data_cols = 2\n",
    "    num_data_cols = exa.meta.input_column_count - num_non_data_cols\n",
    "\n",
    "    df = create_df(ctx, num_non_data_cols, num_data_cols)\n",
    "\n",
    "    # Transform feature data\n",
    "    df = transform_dataframe(df, class_col_name='class',\n",
    "                                fit_transformer=fit_transformer,\n",
    "                                save_path=transformer_path)\n",
    "\n",
    "    # Output data\n",
    "    for i in range(0, df.shape[0]):\n",
    "        out_list = np_numeric_as_scalar(df.iloc[i, :])\n",
    "        ctx.emit(*out_list)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Transform training data\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE TABLE IDA.TRAIN_TRANSFORMED AS\n",
    "    SELECT IDA.DF_TRANSFORM(TRUE, '{transformer_path}', {\", \".join(column_names)}) from IDA.TRAIN\n",
    "\"\"\")\n",
    "conn.execute(sql)\n",
    "\n",
    "# Transform test data\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE TABLE IDA.TEST_TRANSFORMED AS\n",
    "    SELECT IDA.DF_TRANSFORM(FALSE, '{bucketfs_service}{transformer_path}', {\", \".join(column_names)}) from IDA.TEST\n",
    "\"\"\")\n",
    "conn.execute(sql)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "\n",
    "Now that we have transformed the data, we will build a model, which will be used to predict if each instance is an APS failure or not.\n",
    "\n",
    "For this example, we will use a classifier based on the extremely randomized trees (extra-trees) algorithm. This tree-based ensemble method is similar to a random forest, except that the tree splitting is randomized, among other things. This can improve the accuracy as well as the computation time. Details can be found [here](https://orbi.uliege.be/handle/2268/9357).\n",
    "\n",
    "As with most machine learning algorithms, there are multiple parameters which need to be tuned in order to optimize the performance of the classifier for our problem and data. Rather than try many combinations by hand, we will use grid search and cross validation on the training data to find the optimal parameters in the specified subset of parameters.\n",
    "\n",
    "Because searching a large grid can be computationally intensive, a good set of parameter values has already been found using grid search offline. Thus, only a small, coarse subspace of the search grid is used in the example code below so that executing the code does not take too long.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "A good set of parameter values found offline using grid search is the following:<br>\n",
    "n_estimators: 61<br>\n",
    "max_depth: 10<br>\n",
    "class_weight: {0: 1, 1: 89}\n",
    "</div>\n",
    "\n",
    "After the optimal parameters have been found using grid search, an `ExtraTreesClassifier` model will be created using those parameters and saved to an EXABucket for use in the next step, which is training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(f\"ALTER SESSION SET SCRIPT_LANGUAGES='{EXASOL_SCRIPT_LANGUAGES}'\")\n",
    "\n",
    "# EXABucket path to store the classifier\n",
    "classifier_path = \"/udfs/classifier.pkl\"\n",
    "\n",
    "# Script input column descriptions are now the same as output\n",
    "# One class label, numeric data\n",
    "column_types = [\"INT\"] + [\"DOUBLE\"] * (len(column_names) - 1)\n",
    "column_desc = [\" \".join(t) for t in zip(column_names, column_types)]\n",
    "\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE SKLEARN_PYTHON SET SCRIPT\n",
    "IDA.BUILD_MODEL(classifier_path VARCHAR(200), {\", \".join(column_desc)})\n",
    "EMITS (dummy int) AS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycurl\n",
    "import uuid\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Random state to use for reproducibility\n",
    "RAND_STATE = 3\n",
    "\n",
    "# Create DataFrame\n",
    "def create_df(ctx, start_col, num_cols):\n",
    "    df = []\n",
    "    col_names = [col.name for col in exa.meta.input_columns[start_col : num_cols + num_cols]]\n",
    "    while True:\n",
    "        row = []\n",
    "        for i in range(start_col, start_col + num_cols):\n",
    "            row.append(ctx[i])\n",
    "        df.append(row)\n",
    "        if not ctx.next():\n",
    "            break;\n",
    "    df = pd.DataFrame(df, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Upload file to EXABucket\n",
    "def upload_file_bucketfs(model, path):\n",
    "    temp_file = \"/tmp/\" + str(uuid.uuid4().hex + \".pkl\")\n",
    "    joblib.dump(model, temp_file, compress=True)\n",
    "\n",
    "    with open(temp_file, \"rb\") as f:\n",
    "        url = \"https://{EXASOL_BUCKETFS_USER}:{EXASOL_BUCKETFS_PASSWORD}@{EXASOL_BUCKETFS_HOST}\" + path\n",
    "        curl = pycurl.Curl()\n",
    "        curl.setopt(pycurl.URL, url)\n",
    "        curl.setopt(pycurl.SSL_VERIFYPEER, 0)   \n",
    "        curl.setopt(pycurl.SSL_VERIFYHOST, 0)\n",
    "        curl.setopt(curl.UPLOAD, 1)\n",
    "        curl.setopt(curl.READDATA, f)\n",
    "        curl.perform()\n",
    "        curl.close()\n",
    "\n",
    "# Build extra-tree classifier\n",
    "def build_et_classifier(X, class_col_name, model_path=None):\n",
    "    # Convert columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    y = X.loc[:, class_col_name]\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Create classifier\n",
    "    clf = ExtraTreesClassifier(random_state=RAND_STATE, n_jobs=-1)\n",
    "\n",
    "    # Specify parameter search grid\n",
    "    # The grid size is kept small to reduce the computation time\n",
    "    # Good values (known from offline grid search) are:\n",
    "    # 'n_estimators': 61\n",
    "    # 'max_depth': 10\n",
    "    # 'class_weight': {{0: 1, 1: 89}}\n",
    "    param_grid = [{{\n",
    "        'n_estimators': [30, 61],\n",
    "        'max_depth': [5, 10],\n",
    "        'class_weight': [{{0: 1, 1: 89}}]\n",
    "    }}]\n",
    "\n",
    "    # Define scoring metric for grid search from problem description\n",
    "    def ida_score(y, y_pred):\n",
    "        false_preds = y - y_pred\n",
    "        num_false_pos = (false_preds < 0).sum()\n",
    "        num_false_neg = (false_preds > 0).sum()\n",
    "        return -(num_false_pos * 10 + num_false_neg * 500)\n",
    "\n",
    "    ida_scorer = make_scorer(ida_score)\n",
    "\n",
    "    # Search for optimal values in grid using 5-fold cross validation\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=ida_scorer, n_jobs=-1)\n",
    "    grid_search.fit(X_data, y.values.ravel())\n",
    "\n",
    "    # Create new model with optimal parameter values\n",
    "    clf = ExtraTreesClassifier(random_state=RAND_STATE, n_jobs=-1,\n",
    "                                n_estimators=grid_search.best_params_['n_estimators'],\n",
    "                                max_depth=grid_search.best_params_['max_depth'], \n",
    "                                class_weight=grid_search.best_params_['class_weight'])\n",
    "\n",
    "    # Save classifier to EXABucket\n",
    "    if model_path:\n",
    "        upload_file_bucketfs(clf, model_path)\n",
    "\n",
    "def run(ctx):\n",
    "    # Input argument\n",
    "    classifier_path = ctx.classifier_path\n",
    "\n",
    "    # Specify number of non-data (above) and data (everything else) columns\n",
    "    num_non_data_cols = 1\n",
    "    num_data_cols = exa.meta.input_column_count - num_non_data_cols\n",
    "\n",
    "    df = create_df(ctx, num_non_data_cols, num_data_cols)\n",
    "\n",
    "    # Shuffle data\n",
    "    train_set = resample(df, replace=False, random_state=RAND_STATE)\n",
    "\n",
    "    # Build extra-tree classifier\n",
    "    build_et_classifier(train_set, class_col_name='class', model_path=classifier_path)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Build model\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "SELECT IDA.BUILD_MODEL('{classifier_path}', {\", \".join(column_names)}) from IDA.TRAIN_TRANSFORMED\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "After transforming the data and creating the classifier, we will now train it on all the transformed training data. Then, it will be stored in the provided EXABucket for later use during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(f\"ALTER SESSION SET SCRIPT_LANGUAGES='{EXASOL_SCRIPT_LANGUAGES}'\")\n",
    "\n",
    "# EXABucket path to load/store the classifier\n",
    "bucketfs_service = \"/buckets/bucketfs1\"\n",
    "classifier_path = \"/udfs/classifier.pkl\"\n",
    "\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE SKLEARN_PYTHON SET SCRIPT\n",
    "IDA.TRAIN_MODEL(classifier_load_path VARCHAR(200), classifier_save_path VARCHAR(200), {\", \".join(column_desc)})\n",
    "EMITS (dummy int) AS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycurl\n",
    "import uuid\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Random state to use for reproducibility\n",
    "RAND_STATE = 3\n",
    "\n",
    "# Create DataFrame\n",
    "def create_df(ctx, start_col, num_cols):\n",
    "    df = []\n",
    "    col_names = [col.name for col in exa.meta.input_columns[start_col : num_cols + num_cols]]\n",
    "    while True:\n",
    "        row = []\n",
    "        for i in range(start_col, start_col + num_cols):\n",
    "            row.append(ctx[i])\n",
    "        df.append(row)\n",
    "        if not ctx.next():\n",
    "            break;\n",
    "    df = pd.DataFrame(df, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Upload file to EXABucket\n",
    "def upload_file_bucketfs(model, path):\n",
    "    temp_file = \"/tmp/\" + str(uuid.uuid4().hex + \".pkl\")\n",
    "    joblib.dump(model, temp_file, compress=True)\n",
    "\n",
    "    with open(temp_file, \"rb\") as f:\n",
    "        url = \"https://{EXASOL_BUCKETFS_USER}:{EXASOL_BUCKETFS_PASSWORD}@{EXASOL_BUCKETFS_HOST}\" + path\n",
    "        curl = pycurl.Curl()\n",
    "        curl.setopt(pycurl.URL, url)\n",
    "        curl.setopt(pycurl.SSL_VERIFYPEER, 0)   \n",
    "        curl.setopt(pycurl.SSL_VERIFYHOST, 0)\n",
    "        curl.setopt(curl.UPLOAD, 1)\n",
    "        curl.setopt(curl.READDATA, f)\n",
    "        curl.perform()\n",
    "        curl.close()\n",
    "\n",
    "# Train classifier\n",
    "def train(X, class_col_name, model_load_path, model_save_path):\n",
    "    # Convert columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    y = X.loc[:, class_col_name]\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Load model from EXABucket\n",
    "    clf = joblib.load(model_load_path)\n",
    "    clf.fit(X_data, y.values.ravel())\n",
    "\n",
    "    # Save classifier to EXABucket\n",
    "    if model_save_path:\n",
    "        upload_file_bucketfs(clf, model_save_path)\n",
    "\n",
    "def run(ctx):\n",
    "    # Input argument\n",
    "    classifier_load_path = ctx.classifier_load_path\n",
    "    classifier_save_path = ctx.classifier_save_path\n",
    "\n",
    "    # Specify number of non-data (above) and data (everything else) columns\n",
    "    num_non_data_cols = 2\n",
    "    num_data_cols = exa.meta.input_column_count - num_non_data_cols\n",
    "\n",
    "    df = create_df(ctx, num_non_data_cols, num_data_cols)\n",
    "\n",
    "    # Shuffle data\n",
    "    train_set = resample(df, replace=False, random_state=RAND_STATE)\n",
    "\n",
    "    # Train the classifier\n",
    "    train(train_set, class_col_name='class',\n",
    "            model_load_path=classifier_load_path,\n",
    "            model_save_path=classifier_save_path)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Train model\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "SELECT IDA.TRAIN_MODEL('{bucketfs_service}{classifier_path}', '{classifier_path}', {\", \".join(column_names)}) from IDA.TRAIN_TRANSFORMED\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "After training the classifier, we will now test it using the transformed test data.\n",
    "\n",
    "The model that was saved during training will now be loaded from the EXABucket and used to predict the classes of the test data. The returned data, which is stored in the table `TEST_PREDICTIONS`, will be the predicted classes (first column) joined to the transformed test data. In this way, we can ensure that the predicted and real classes remain properly ordered/linked for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(f\"ALTER SESSION SET SCRIPT_LANGUAGES='{EXASOL_SCRIPT_LANGUAGES}'\")\n",
    "\n",
    "# Create script output column descriptions\n",
    "# Two class labels, numeric data\n",
    "out_column_types = [\"INT\"] * 2 + [\"DOUBLE\"] * (len(column_names) - 1)\n",
    "out_column_desc = [\" \".join(t) for t in zip([\"class_pred\"] + column_names, out_column_types)]\n",
    "\n",
    "# EXABucket path to load the classifier\n",
    "bucketfs_service = \"/buckets/bucketfs1\"\n",
    "classifier_path = \"/udfs/classifier.pkl\"\n",
    "\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE SKLEARN_PYTHON SET SCRIPT\n",
    "IDA.TEST_MODEL(classifier_path VARCHAR(200), {\", \".join(column_desc)})\n",
    "EMITS ({\", \".join(out_column_desc)}) AS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Create DataFrame\n",
    "def create_df(ctx, start_col, num_cols):\n",
    "    df = []\n",
    "    col_names = [col.name for col in exa.meta.input_columns[start_col : num_cols + num_cols]]\n",
    "    while True:\n",
    "        row = []\n",
    "        for i in range(start_col, start_col + num_cols):\n",
    "            row.append(ctx[i])\n",
    "        df.append(row)\n",
    "        if not ctx.next():\n",
    "            break;\n",
    "    df = pd.DataFrame(df, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Convert NumPy values to native Python type\n",
    "def np_numeric_as_scalar(x):\n",
    "    return [np.asscalar(x[i]) for i in range(0, len(x))]\n",
    "\n",
    "# Test classifier\n",
    "def test(X, class_col_name, model_path=None):\n",
    "    # Convert columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Load model from EXABucket\n",
    "    clf = joblib.load(model_path)\n",
    "\n",
    "    # Predict classes of test data\n",
    "    return clf.predict(X_data)\n",
    "\n",
    "def run(ctx):\n",
    "    # Input argument\n",
    "    classifier_path = ctx.classifier_path\n",
    "\n",
    "    # Specify number of non-data (above) and data (everything else) columns\n",
    "    num_non_data_cols = 1\n",
    "    num_data_cols = exa.meta.input_column_count - num_non_data_cols\n",
    "\n",
    "    df = create_df(ctx, num_non_data_cols, num_data_cols)\n",
    "\n",
    "    # Test the classifier\n",
    "    y_pred = test(df, class_col_name='class', model_path=classifier_path)\n",
    "\n",
    "    # Add class predictions as first column of test DataFrame\n",
    "    df_pred = (pd.DataFrame(y_pred, columns=['class_pred'])).join(df)\n",
    "\n",
    "    # Convert columns to numeric data types\n",
    "    df_pred = df_pred.apply(pd.to_numeric)\n",
    "\n",
    "    # Output data\n",
    "    for i in range(0, df_pred.shape[0]):\n",
    "        out_list = np_numeric_as_scalar(df_pred.iloc[i, :])\n",
    "        ctx.emit(*out_list)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Test model\n",
    "sql = textwrap.dedent(f\"\"\"\\\n",
    "CREATE OR REPLACE TABLE IDA.TEST_PREDICTIONS AS\n",
    "    SELECT IDA.TEST_MODEL('{bucketfs_service}{classifier_path}', {\", \".join(column_names)}) from IDA.TEST_TRANSFORMED\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(sql)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Now that we have the predicted classes labels of the test data, we can just compare them to the real class labels to evaluate how well our classifier performs.\n",
    "\n",
    "We use the `ida_cost` method as the performance metric, which was defined in the problem description. Additionally, the confusion is also displayed to see how well each class was classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define cost function from the problem description\n",
    "def ida_cost(y, y_pred):\n",
    "    false_preds = y - y_pred\n",
    "    num_false_pos = (false_preds < 0).sum()\n",
    "    num_false_neg = (false_preds > 0).sum()\n",
    "    return num_false_pos * 10 + num_false_neg * 500\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "# Get predicted and real class labels\n",
    "test_preds = conn.export_to_pandas(\"SELECT CLASS_PRED, CLASS FROM IDA.TEST_PREDICTIONS\")\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "y_pred = test_preds.loc[:, 'CLASS_PRED']\n",
    "y = test_preds.loc[:, 'CLASS']\n",
    "\n",
    "# Examine the results\n",
    "confusion_mat = confusion_matrix(y, y_pred)\n",
    "confusion_matrix_df = pd.DataFrame(confusion_mat, index=['neg', 'pos'], columns=['neg_pred', 'pos_pred'])\n",
    "\n",
    "print(\"Cost:\", ida_cost(y, y_pred),\"\\n\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Having gone through this simple, real-world example, hopefully you can now see how you can integrate your machine learning algorithms and data science processes directly into Exasol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
